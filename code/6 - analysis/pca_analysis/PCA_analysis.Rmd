---
title: "PCA_analysis"
author: "Yuxuan"
date: "15/06/2021"
output: html_document
---

### PCA computation


#### Background:
PCA assumes that the directions with the largest variances are the most "important"
It can be used to reduce the dimensionnality of the data

The goal of this analysis is to :

- identify correlated variables

  - does amenity scores correlated with rent/proxy data/population?
  
- reduce the dimensionnality of the data

  - what features are important?

- identify the hidden pattern in the data set.
 
  - which area/city are similar to each other?




#### 1) load the library

```{R message=FALSE, warning=FALSE, include=TRUE}
# wrangling/convenience
library(tidyverse)
library(glue)
library(stringr)
library(sf)
library(caret)
library(data.table)

# For pretty knitting
library(lemon)
knit_print.data.frame <- lemon_print
knit_print.tbl <- lemon_print
knit_print.summary <- lemon_print


# For PCA
library(missMDA)
library(FactoMineR)
library("factoextra")
library(tidyverse)

# corr plot
library("cowplot")
library("corrplot")
```


#### 2) Import the dataset
```{R}
#df_pca<-fread(file.path('../../data/clean', '/pca_data.csv'))
df_pca<-fread(file.path('../../../data/3_computed', '/unsupervised_data.csv'))
```



#### 3) Data cleaning

```{R}
# select only numeric column



df_pca<-data.frame(column_to_rownames(df_pca, var = "V1"))



colnames(df_pca)<-c("AVG SCORE","PROX TRANSIT","POPULATION","AMENITIES","AVG TIME","BUS STOPS","AVG BUS FREQ","AVG AMENITY POPULARITY")

df.num<-df_pca%>%select(where(is.numeric))

```

###

```{R}




M<-cor(df.num)
corrplot(M, type="upper")


# mat : is a matrix of data
# ... : further arguments to pass to the native R cor.test function
cor.mtest <- function(mat, ...) {
    mat <- as.matrix(mat)
    n <- ncol(mat)
    p.mat<- matrix(NA, n, n)
    diag(p.mat) <- 0
    for (i in 1:(n - 1)) {
        for (j in (i + 1):n) {
            tmp <- cor.test(mat[, i], mat[, j], ...)
            p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
        }
    }
  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
  p.mat
}
# matrix of the p-value of the correlation
p.mat <- cor.mtest(df.num)

head(p.mat[, 1:5])
#title <- "Transit feature p-value significance "
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(M, method="color", col=col(200),  
         type="upper", order="hclust", 
         addCoef.col = "black", # Add coefficient of correlation
         tl.col="black", tl.srt=45, #Text label color and rotation
         #title=title,
         # Combine with significance
         p.mat = p.mat, sig.level = 0.01, insig = "blank", 
         # hide correlation coefficient on the principal diagonal
         diag=FALSE 
         )
```



#### 4) PCA computing

Using eigenvalues to determine number of component to retian 
```{R}
res.pca <- prcomp(na.omit(df.num), scale = TRUE)
eig.val <- get_eigenvalue(res.pca)

eig.val
head(eig.val,6)[2]%>%sum()
```

Visualize eigenvalues (scree plot). 
Show the percentage of variances explained by each principal component.
```{R}
fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 45))
```

First six principal component retain 93.64864% of the info


##### Results

```{R}
var <- get_pca_var(res.pca)
var
```
var$cos2: represents the quality of representation for variables on the factor map. Itâ€™s calculated as the squared coordinates: var.cos2 = var.coord * var.coord.





##### Quality of representation

The quality of representatin of the variables on factor map is call cos2

Visualize the cos2 of varible on all dimensions 
```{R}
head(var$cos2, 4)
corrplot(var$cos2, is.corr=FALSE)
```
Total cos2 of variables on Dim.1 and Dim.2

```{R}
fviz_cos2(res.pca, choice = "var", axes = 1:2)

```

By cos2
```{R}
#Color by cos2 values: quality on the factor map
fviz_pca_var(res.pca, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE # Avoid text overlapping
             )


```

- A high cos2 indicates a good representation of the variable on the principal component.(far from the center )

- A low cos2 indicates that the varibale is not perfectly represented by the principal component.(close to the center)

##### Contributions of variables to PCs

- Variables that are correlated with PC1  and PC2 are the most important in explaining the variability in the data set.

- Variables that do not correlated with any PC or correlated with the last dimensions are variables with low contribution and might be removed to simplify the overall analysis.

```{R}
par(mfrow=c(1,2))
# Contributions of variables to PC1
fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
# Contributions of variables to PC2
fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)

```


By Crontribution 
```{R}
#Graph of variables. Positive correlated variables point to the same side of the plot. Negative correlated variables point to opposite sides of the graph.


fviz_pca_var(res.pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```

-Positively correlated variables are grouped together.
-Negatively correlated variables are positioned on opposite sides of the plot origin (opposed quadrants).

The distance between variables and the origin measures the quality of the variables on the factor map. Variables that are away from the origin are well represented on the factor map. (see ref)



###### The difference between cos2 and contribution is that cos2 is calculated by var.cos2 = var.coord * var.coord and contribution is calculated by (var.cos2 * 100) / (total cos2 of the component).


##### Group by individual (Cities)

We may want to see which cities are similar to each other
It could scale up to national wide to see which cities in Canada are similar 



```{R}
# graph of individuals
fviz_pca_ind(res.pca,
             col.ind = "cos2", # Color by the quality of representation
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```



##### Biplot of individuals and variables

```{R}
fviz_pca_biplot(res.pca, repel = TRUE,
                col.var = "#2E9FDF", # Variables color
                col.ind = "#696969"  # Individuals color
                )
```


```{R}
write.csv(df_pca,"../../../dashboard_deployed/datatable/pca_data.csv")

```

##### Reference
http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/


