---
title: "Travel Time Matrix to Maps"
output: html_notebook
---

## Notebook Purpose

This notebook serves to summarize the entire visualization process going from 
the travel time matrix to the visualizations. That involves the following 
sections:

1) Travel Time Matrix Wrangling
2) Score Computation
3) Isochrone Computation
4) Dataset Wrangling Part II (NA Insertion)
5) Interactive Visualization
6) Map HTML Exports

## 0) Useful Libraries

```{r message=FALSE, warning=FALSE, include=TRUE}
# import custom scoring, cleaning, and visualization functions
source('functions.R')

# wrangling/convenience
library(tidyverse)
library(glue)
library(stringr)
library(sf)
library(data.table)

# visualization
library(leaflet)
library(mapview); mapviewOptions(platform = 'leafgl')
#library(ggplot2)
#library(RColorBrewer)
#library(scales)
#library(lattice)

# For pretty knitting
library(lemon)
knit_print.data.frame <- lemon_print
knit_print.tbl <- lemon_print
knit_print.summary <- lemon_print
```





## 1) Data Wrangling

### Import all dissemination block data
```{r}
# import dissemination blocks and keep id and pop columns
origins <- fread(file.path("../../data/clean", "vancouver_db.csv"))[, .(id, pop)]
origins$pop <- str_replace_all(origins$pop, ',', '')

# change col types
origins$pop <- as.numeric(origins$pop)  
origins$id <- as.factor(origins$id)  
n_origins <- nrow(origins)
paste('Origin Rows: ', n_origins)

# Peek
head(origins)
```


### Import all amenity data
```{r}
# import amenities (Cultural/Art facilities)
destinations <- fread(file.path("../../data/clean", "vancouver_facilities_2.csv"))

# see summary counts of each amenity
destinations %>% group_by(type) %>% summarise(count = n()) %>% arrange(desc(count))

# clean amenities / filter types to keep 4 most frequent amenities
target_amenities <- c('gallery', 'museum', 'library or archives', 'theatre/performance and concert hall')
destinations <- destinations %>% filter(type %in% target_amenities)
# keep only id and type columns
destinations <- destinations[ , .(id, type)]
# change col types
destinations$type <- as.factor(destinations$type)
destinations$id <- as.factor(destinations$id)  

n_amenities <- nrow(destinations)
paste('Destinations: ', n_amenities)
head(destinations)
```


### Import the travel time matrix
```{r kable.opts=list(caption='Summary Table')}
# import travel time matrix
ttm <- fread(file.path('../../data/clean', 'ttm.csv'))

# convert Ids to  factor
ttm$fromId <- as.factor(ttm$fromId)
ttm$toId <- as.factor(ttm$toId)

## Replace travel times less than 1 minute to 1 minute
# This is done to prevent infinity values in the scoring since
# 1 minute is still a reasonable time for trips in the 0 - 1 min range.
ttm$avg_time <- pmax(ttm$avg_time, 1)

# peek
head(ttm)
```


### Import amenity weights:
```{r}
# import amenity weights
amenity_wts <- read.csv('../../data/amenity_weights/amenity_wts.csv')
amenity_wts

# clean weights
amenity_wts <- amenity_wts[, c('id', 'Index')]
names(amenity_wts) <- c('id', 'weight')
amenity_wts$id <-  as.factor(amenity_wts$id)

amenity_wts %>% group_by(id) %>% summarize(n = n()) %>% arrange(desc(n))


# Check: are all the ttm amenity IDs in the weighted IDs set?
check <- all(unique(ttm$toId) %in% unique(amenity_wts$id))
# needs to be true for the join to work
paste('Are all the ttm amenity IDs in the weighted IDs set? =', check)

```

### Fixed unequal number of ttm amenity IDs in the weighted IDs

```{R}
# ttm ids that appear in the weights ids
ttm_id_in_wts <- unique(ttm$toId)[unique(ttm$toId) %in% unique(amenity_wts$id)]

# subset these id that not in amenity wts
ttm_id_not_in_wts <- unique(subset(ttm, !(toId %in% ttm_id_in_wts))$toId)
ttm_id_not_in_wts <- as.data.frame(list("id_not_in_wts" = ttm_id_not_in_wts))

paste('Number of id that not in amenity wts ', ttm_id_not_in_wts %>% nrow())


# assign minimum weight on those places
ttm_id_not_in_wts$weight <- min(amenity_wts$weight)
colnames(ttm_id_not_in_wts)[1] <- "id"

# add it to amenity weights
amenity_wts <- rbind(amenity_wts, ttm_id_not_in_wts)

# Check: are all the ttm amenity IDs in the weighted IDs set?
check <- all(unique(ttm$toId) %in% unique(amenity_wts$id))
paste('Are all the ttm amenity IDs in the weighted IDs set? (needs to be true for the join to work) =', check)

```

### Join ttm, destination types, and amenity weights
```{r}

if (check == TRUE){
  # join amenity weights to amenity types
  destinations <- left_join(destinations, amenity_wts, by = c('id' = 'id'))
  
  # join to ttm
  # use left join since we only care to keep existing amenities in the ttm
  ttm <-  left_join(ttm, destinations, by = c('toId' = 'id'))
}

head(ttm)

# descriptive info: how many origins actually have transit accessibility
paste('Origins considered:', round((length(unique(ttm$fromId))/n_origins)*100, 2), '%')
paste('Destinations considered:', round(length(unique(ttm$toId))/n_amenities*100, 2), '%')
paste('Rows = ', nrow(ttm))

```


### Isochrone Frame and Types
```{r}

# only consider the nearest amenity
isochrone_frame <- ttm %>%
  group_by(fromId, type) %>%
  summarise(avg_time = min(avg_time))

# isochrone frame
isochrone_frame$time_groups <-  with(isochrone_frame,
                                   ifelse(avg_time < 15, "15",
                                   ifelse(avg_time < 30, "30",
                                   ifelse(avg_time < 45, "45",
                                   ifelse(avg_time < 60, "60",
                                   ifelse(avg_time < 75, "75",
                                   ifelse(avg_time < 90, "90", "90+")))))))

isochrone_frame <- isochrone_frame[, c(1,2,4)]

head(isochrone_frame)

# see how many trips in each time_group
isochrone_frame %>% group_by(time_groups) %>% summarise(n = n()) -> t_groups
plot(t_groups$n, type = 'b')
```


### Import the dissemination block shape file
```{r}
canada_shape <- st_read("../../data/census2016_DBS_shp/DB_Van_CMA/DB_Van_CMA.shp", stringsAsFactors = FALSE)

# select a greater metropolitan area
metropolitan_area <- "Vancouver"

# filter columns and rows
vancouver_shape <- data.frame(canada_shape[which(canada_shape$CMANAME == metropolitan_area), c(1, 28)])

# id to factor
vancouver_shape$DBUID <- as.factor(vancouver_shape$DBUID)

paste('Rows = ', nrow(vancouver_shape))
head(vancouver_shape)
```






## 2) Score Computation

Notes: 

- We don't scale the data before score computation because we care about the
scale of time and standard deviation. We should use their values as is, 
otherwise scaling will give them equal weighing again when this is a bad
assumption.
- Log normalizing the score isn't important as the score visualization depends 
on the quantiles taken from the distribution of scores. Since log only shifts 
values, the visualizations will be identical.



```{r message=FALSE, warning=FALSE}

score_list <- list()
score_list_weighted<-list()
i <- 1

for (n in 1:4) {
  
  # we want average of all times so N must be undefined
  if (n == 4) { n <- NULL }
  
  # unweighted score
  score <- sum_score_fxn(ttm, nearest_n = n, weight = FALSE, log_normalize_score = FALSE)
  # weighted score
  score_weighted <- sum_score_fxn(ttm, nearest_n = n, weight = TRUE, log_normalize_score = FALSE)
  
  # append
  score_list[[i]] <- score
  score_list_weighted[[i]]<-score_weighted
  
  i <- i+1
}

scores_long_unweighted <- rbindlist(score_list) %>% arrange(fromId, nearest_n)
scores_long_weighted <- rbindlist(score_list_weighted) %>% arrange(fromId, nearest_n)

scores_long <- rbindlist(list(scores_long_weighted, scores_long_unweighted))
```



## 3) Dataset Wrangling Part II (NA Insertion)

Each origin(fromId) should have x different scores where x is defined by:

x = 2 weight options * 4 amenity options * 4 nearest options = 32

In addition to filling the empty NA rows for included IDs, there are also IDs
that need to be re-added as not a single time was computed for these IDs.


```{r}
# values per ID (32)
x <- 2*4*4

paste0('SCORES FRAME')

# HOW MANY ROWS TO FILL IN SCORES?
N <- uniqueN(scores_long$fromId) * x # expected rows
paste(glue('{nrow(scores_long)} of {N} rows filled ({round((nrow(scores_long) / N)*100, 2)}%)'))
paste(N - nrow(scores_long), 'to fill.')

cat(paste0('\n')) # line break

# HOW MANY ROWS TO ADD IN SCORES?
# existing IDs that weren't included in ttm
missing_blocks <- array(setdiff(origins$id, scores_long$fromId))
total_expected <- nrow(scores_long) + length(missing_blocks) * x
paste(glue('{nrow(scores_long)} of {total_expected} rows filled ({round((nrow(scores_long) / total_expected)*100, 2)}%)'))
paste(length(missing_blocks)*x, 'to add')

cat(paste0('\n')) # line break
paste0('ISOCHRONE FRAME')

# HOW MANY ROWS TO FILL IN ISOCHRONE?
N <- uniqueN(isochrone_frame$fromId) * 4 # expected rows
paste(glue('{nrow(isochrone_frame)} of {N} rows filled ({round((nrow(isochrone_frame) / N)*100, 2)}%)'))
paste(N - nrow(isochrone_frame), 'to fill.')

cat(paste0('\n')) # line break

# HOW MANY ROWS TO ADD IN ISOCHRONE?
# existing IDs that weren't included in ttm
missing_blocks <- array(setdiff(origins$id, isochrone_frame$fromId))
total_expected <- nrow(isochrone_frame) + length(missing_blocks) * 4 # only 4 values per origin
paste(glue('{nrow(isochrone_frame)} of {total_expected} rows filled ({round((nrow(isochrone_frame) / total_expected)*100, 2)}%)'))
paste(length(missing_blocks)*4, 'to add')


```

```{r}
# fill NA for all existing origin IDs
filled_scores_long <- NA_table_filler(scores_long)

# add remaining NAs to filled_scores_long
all_scores_long <- NA_table_filler(filled_scores_long,
                                   custom_idx = missing_blocks)



# fill NA for all existing origin IDs
filled_isochrone_frame <- NA_table_filler(isochrone_frame,
                                          isochrone = TRUE)
# add missing NAs to isochrone frame
all_isochrone_frame <- NA_table_filler(filled_isochrone_frame,
                                       custom_idx = missing_blocks,
                                       isochrone=TRUE)

```

```{r}
# check isochrones
# there should be 4 counts per fromId
iso_id_counts <- all_isochrone_frame %>% group_by(fromId) %>% summarise(n = n())
unique(iso_id_counts$n)

# check scores
# there should be 32 counts per fromId
score_id_counts <- all_scores_long %>% group_by(fromId) %>% summarise(n = n())
unique(score_id_counts$n)

```


Now lets add population data to the scores and isochrone frame

```{r}
# right join with origins to include origins without transit access

all_scores_long <- right_join(all_scores_long, origins, by = c('fromId' = 'id'))
all_isochrone_frame <- right_join(all_isochrone_frame, origins, by = c('fromId' = 'id'))

```

```{r}

## Export checkpoint
write.csv(all_scores_long, '../../data/html_mapping_data/scores_long.csv', row.names = FALSE)
write.csv(all_isochrone_frame, '../../data/html_mapping_data/isochrone_frame.csv', row.names = FALSE)

```


## 4) Interactive Visualization
```{r}
# join factor and geometry data 
scores_viz_frame <- left_join(vancouver_shape, all_scores_long, by = c('DBUID' = 'fromId'))
isochrone_viz_frame <- left_join(vancouver_shape, all_isochrone_frame, by = c('DBUID' = 'fromId'))

# convert back to sf object
scores_viz_frame_sf <- st_as_sf(scores_viz_frame)
isochrone_viz_frame_sf <- st_as_sf(isochrone_viz_frame)

# convert to st object
scores_viz_frame_st <- st_transform(scores_viz_frame_sf, crs = 4326)
isochrone_viz_frame_st <- st_transform(isochrone_viz_frame_sf, crs = 4326)

```


```{r}
# test if curious
#map_maker_scores(scores_viz_frame_st, 'gallery',"no","1", view_map = TRUE)
#map_maker_isochrone(data = isochrone_viz_frame_st, "museum", view_map = TRUE)

```


## 5) Map HTML Exports

```{r}
# exclue NA in type
type_name <- unique(scores_viz_frame_st$type)

for (amenity in type_name) { 
  
  # 4 isochrone maps
  map_maker_isochrone(data = isochrone_viz_frame_st, amenity, output_dir = '../../data/html_maps/isochrone_maps')

  for (weight in unique(scores_viz_frame_st$weight)) {
    for (nearest_n in unique(scores_viz_frame_st$nearest_n)) {
      
      # 32 others
      map_maker_scores(data = scores_viz_frame_st, amenity, weight, nearest_n,
                       output_dir = '../../data/html_maps/score_maps')
      
    }
  }
}
```


































