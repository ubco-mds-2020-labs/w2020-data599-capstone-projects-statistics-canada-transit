---
title: "Amenity_index_Function"
author: "Yuxuan"
date: "25/05/2021"
output:
  html_document: default
  pdf_document: default
---


***Import libraries***
```{r message=FALSE, warning=FALSE}
library(dplyr)
library(readr)
library(ggplot2)
library(tidyr)
library(imputeTS)
library(corrplot)
library(qwraps2)
options(qwraps2_markup='markdown')
library(hablar)
```

**Import data**

Requires two dataset; *google_reviews_poi_with_hours.csv* and *vancouver_facilities_2.csv*
```{R message=FALSE, warning=FALSE}

### set w2020-data599 as working directory
library_obj<-read.csv("data/raw/public_library_data.csv",fileEncoding="latin1")
review_poi<-read_csv("data/clean/google_reviews_poi_with_hours.csv")
van_poi<-read_csv("data/clean/vancouver_facilities_2.csv")
# import travel time matrix
ttm <- read_csv('data/clean/ttm.csv')
```





```{R}
#Merge review dataset with vancouver point of interest
left_join(review_poi,van_poi,by=c("poi_name"="name"))%>%distinct()->merged_data
```



**Convert the data to numeric **
```{R message=FALSE, warning=FALSE}
merged_data%>% convert(num(Rating, Total_Review,open_days,Total_hours))->merged_data

```

**Number of amenity in each type of arts facility **
```{R}
merged_data%>%group_by(type)%>%count()
```
Our primary interest would be gallery(n=99),library(n=86),museum(n=92) and theatre(n=75)



##### visualize missing value 
```{R}
merged_data%>%select(poi_name,Name,type,open_days,Total_hours,Rating,Total_Review,lat,lon)->df

colnames(df)
colnames(df) <- c("POI Name", "Google Place Name","Type","Open Days","Open Hours","Rating","Total Review","lat","lon")
df[df == 0] <- NA
missing_percentage<-colMeans(is.na(df))

missing.values <- df%>%
  gather(key = "key", value = "val") %>%
  mutate(isna = is.na(val)) %>%
  group_by(key) %>%
  mutate(total = n()) %>%
  group_by(key, total, isna) %>%
  summarise(num.isna = n()) %>%
  mutate(pct = num.isna / total * 100)
missing.values
df

levels <-(missing.values  %>% filter(isna == T) %>% arrange(desc(pct)))$key

label_txt<-missing.values%>% filter(isna == T)%>%select(pct)

label_txt$pct<-floor(label_txt$pct)

percentage.plot <- missing.values %>%
      ggplot() +
        geom_bar(aes(x = reorder(key, desc(pct)), 
                     y = pct, fill=isna), 
                 stat = 'identity', alpha=0.8) +
     geom_text(data=label_txt,aes(label =pct,x= reorder(key, desc(pct)),y=pct), size = 3,hjust=-1.1)+
      scale_x_discrete(limits = levels) +
      scale_fill_manual(name = "", 
                        values = c('steelblue', 'tomato3'), labels = c("Present", "Missing")) +
      coord_flip() +
      labs(title = "Percentage of missing values", x =
             'Variable', y = "% of missing values")

p<-percentage.plot+ theme_minimal()
p

ggsave("amenity_missing.png", width = 5, height = 2)
```


**data cleaning**

```{R,options(digits = 2)}
van_list = c('Richmond','Vancouver','Burnaby','Township of Langley','North Vancouver','New Westminster','Maple Ridge','Port Moody','Surrey','Langley',
            'Coquitlam','Delta','Fort Langley','White Rock','Pitt Meadows','Port Coquitlam', 'West Vancouver')
library_obj


# select those only in big vancouver area
library_obj%>%filter(CITY%in%van_list)->library_obj

# select column and rename them

colnames(library_obj)

library_obj%>%select('BRANCH_UNIQUE_ID', 'POSTAL_CODE', 'LATITUDE', 'LONGITUDE', 'MTLS_OUTLET', 'VISITS_B', 'FLOORSPACE.', 'HRS_OPEN.', 'DAYS_OPEN.')->lib_df

colnames(lib_df)<-c('Branch_Id', 'Postal_Code', 'lat', 'lon', 'Total_Volumes', 'Annual_Visitors', 'Total_Space', 'Hrs_per_Year', 'Days_per_Year')

# check nas
colMeans(is.na(lib_df))


  


  
  
df%>%filter(Type=="library or archives")->df_lib


# left join 

#round to 2 decimal

df_lib%>% convert(num(lat,lon))->df_lib
df_lib$lat<-round(df_lib$lat,2)
df_lib$lon<-round(df_lib$lon,2)
lib_df$lat<-round(lib_df$lat,2)
lib_df$lon<-round(lib_df$lon,2)
df_lib<-na_mean(df_lib)
merged_lib<-left_join(df_lib,lib_df,by = c("lat", "lon"))
merged_lib%>%select("POI Name","Open Days","Open Hours", "Rating","Total Review","Total_Volumes","Annual_Visitors","Total_Space" )->merged_lib
merged_lib%>% convert(num(Total_Volumes,Annual_Visitors,Total_Space))->merged_lib



# exclude the name
n_features<-merged_lib%>%rowwise()%>%apply(1,function(x) sum(is.na(x)==F))

#normaliz with library
normalize <- function(x) {
return ((x - min(x,na.rm = T)) / (max(x,na.rm = T) - min(x,na.rm = T)))
  }
  
  

norm_lib<-merged_lib%>%mutate_if(is.numeric, normalize)
  
norm_lib$n_features<-n_features-1
norm_lib%>%rowwise() %>%mutate(Total_features=sum(`Open Days`,`Open Hours`,Rating,`Total Review`,Total_Volumes,Annual_Visitors,Total_Space,na.rm=TRUE),index=Total_features/n_features)->norm_lib




colnames(lib_poi)
norm_lib%>%select(`POI Name`,index)->lib_poi
colnames(lib_poi)<-c("poi_name","Index" )


```
### Weight Index function 

Weight index function takes two arguments, *data* and *type*. *data* should contain the info of amenities such as poi_name,pid,Rating,Total_review,opening_hours,opening_days and total hours. The second argument is the amenity type of interest, for example, museum, gallery, library or archives,etc. Once the function has been called, it returns a list that contains the dataset with weighted amenity index,density plots, correlation plot with corresponding features.



$$Normalize_i = \frac{Value_i-min(Value)}{max(Value)-min(Value)}$$

**weight index funciton with plot**
```{R}
weight_index<-function(data,Amenity="museuem"){
  data%>%filter(type==Amenity)->poi_type
  # select relevent features 
  poi_type%>%select(poi_name,open_days,Total_hours,Rating,Total_Review)->poi_type
  # check number of missing data
  poi_type[poi_type == 0] <- NA
  missing_percentage<-colMeans(is.na(poi_type))
  
  # fill NA with column mean 
  poi_type<-na_mean(poi_type)
  # summary table of interest
  summary<- list(
  "Rating"=list(
    "min"= ~ min(Rating,na.rm = TRUE),
    "max"= ~ max(Rating,na.rm = TRUE),
    "mean"= ~ mean(Rating,na.rm = TRUE)),
  
  "Total_Review"=list(
    "min"= ~ min(Total_Review,na.rm = TRUE),
    "max"= ~ max(Total_Review,na.rm = TRUE),
    "mean"= ~ mean(Total_Review,na.rm = TRUE)),
  
  "Total_hours"=list(
    "min"= ~ min(Total_hours,na.rm = TRUE),
    "max"= ~ max(Total_hours,na.rm = TRUE),
    "standard deviation"= ~ sd(Total_hours,na.rm = TRUE),
    "mean"= ~ mean(Total_hours,na.rm = TRUE)),
  
  "Open_days"=list(
    "min"= ~ min(open_days,na.rm = TRUE),
    "max"= ~ max(open_days,na.rm = TRUE),
     "standard deviation"= ~ sd(open_days,na.rm = TRUE),
    "mean"= ~ mean(open_days,na.rm = TRUE))
  )


  whole<-summary_table(poi_type,summary)
  
  # compute correlation matrix for numeric features
  cor_matrix<-cor(poi_type[-1], use = "complete.obs")
  corrplot<-corrplot(cor_matrix, method =  "number")
  
  
  
  # normalize the features 
  normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
  }
  
  
  
  Norm_poi<-poi_type%>%mutate_if(is.numeric, normalize)
  
  # Navie weighted index 
Norm_poi%>%mutate(Index=(open_days+Total_hours+Rating+Total_Review)/4)->Norm_poi
  
  # plot density of each feature
  
  p1<-plot(density(unlist(Norm_poi[,2])), main = "Normalized Open Days Distribution")
  p1_1<-plot(hist(unlist(poi_type[,2])), main = "Unnormalized Open Days Distribution")
  p2<-plot(density(unlist(Norm_poi[,3])), main = 'Normalized Operation Hours Distribution')
  p3<-plot(density(unlist(Norm_poi[,4])), main = 'Nnormalized Rating Distribution')
  p4<-plot(density(unlist(Norm_poi[,5])), main = 'Nnormalized Total Review Distribution')
  p5<-plot(density(unlist(Norm_poi[,6])), main = 'Nnormalized Index Distribution')
  
  
  #
 result<-list(missing_percentage=missing_percentage,corrplot=corrplot,summary=whole,data=Norm_poi,plot_normalized_days=p1,plot_normalized_hours=p2,plot_normalized_rating=p3,plot_normalized_review=p4)
 return(result)
  
} 



```

**weight index function without plots**

```{R}
weights<-function(data,Amenity="museuem"){
  data%>%filter(type==Amenity)->poi_type
  # select relevent features 
  poi_type%>%select(poi_name,open_days,Total_hours,Rating,Total_Review)->poi_type
  # check number of missing data
  poi_type[poi_type == 0] <- NA
  missing_percentage<-colMeans(is.na(poi_type))
  
  # fill NA with column mean 
  poi_type<-na_mean(poi_type)

  # normalize the features 
  normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
  }
  
  Norm_poi<-poi_type%>%mutate_if(is.numeric, normalize)
  
  # Navie weighted index 
  Norm_poi%>%mutate(Index=(open_days+Total_hours+Rating+Total_Review)/4)->Norm_poi
  Norm_poi%>%select(poi_name,Index)->return_df
  return(return_df)

  
}

```


#### Select the point of interest 

```{r echo=TRUE, fig.show='hide'}
# 
poi_int<-c("museum","gallery","theatre/performance and concert hall")

df<-weights(merged_data,Amenity="museum")%>%as.data.frame()
colnames(df)

df<-df[F,]



for(name in poi_int){
  tem<-weights(merged_data,Amenity=name)
  df<-rbind(invisible(tem),df)
}



```

```{R}

df
```



**merge score with library**

```{R}

# convert lat and lon into numeric 

rbind(df,lib_poi)->df_poi
poi_index<-left_join(df_poi,van_poi,by=c("poi_name"="name"))


```

```{R}
# clean weights
amenity_wts <- poi_index[, c('id', 'Index')]
names(amenity_wts) <- c('id', 'weight')
amenity_wts$id <-  as.factor(amenity_wts$id)
amenity_wts[!duplicated(amenity_wts$id), ]->amenity_wts
amenity_wts %>% group_by(id) %>% summarize(n = n()) %>% arrange(desc(n))

# Check: are all the ttm amenity IDs in the weighted IDs set?
check <- all(unique(ttm$toId) %in% unique(amenity_wts$id))
# needs to be true for the join to work
paste('Are all the ttm amenity IDs in the weighted IDs set? =', check)


```

### Fixed unequal number of ttm amenity IDs in the weighted IDs

```{R}
# convert Ids to  factor
ttm$fromId <- as.factor(ttm$fromId)
ttm$toId <- as.factor(ttm$toId)
# ttm ids that appear in the weights ids
ttm_id_in_wts <- unique(ttm$toId)[unique(ttm$toId) %in% unique(amenity_wts$id)]

# subset these id that not in amenity wts
ttm_id_not_in_wts <- unique(subset(ttm, !(toId %in% ttm_id_in_wts))$toId)
ttm_id_not_in_wts <- as.data.frame(list("id_not_in_wts" = ttm_id_not_in_wts))

paste('Number of id that not in amenity wts ', ttm_id_not_in_wts %>% nrow())


# assign minimum weight on those places
ttm_id_not_in_wts$weight <- min(amenity_wts$weight)
colnames(ttm_id_not_in_wts)[1] <- "id"

# add it to amenity weights
amenity_wts <- rbind(amenity_wts, ttm_id_not_in_wts)



amenity_wts<-amenity_wts[complete.cases(amenity_wts),]
# Check: are all the ttm amenity IDs in the weighted IDs set?
check <- all(unique(ttm$toId) %in% unique(amenity_wts$id))
paste('Are all the ttm amenity IDs in the weighted IDs set? (needs to be true for the join to work) =', check)


amenity_wts

```



##### Export csv 
amenity_wts file contians id and wieghts 
```{r echo=TRUE}
#poi_index<-left_join(df,van_poi,by=c("poi_name"="name"))
write.csv(amenity_wts,'data/amenity_weights/amenity_wts.csv',row.names = FALSE)
```
