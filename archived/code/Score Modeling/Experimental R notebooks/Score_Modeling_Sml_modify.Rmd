---
title: "Score Computation Notebook"
output:
  pdf_document: default
  html_notebook: default
  html_document:
    df_print: paged
---

## Changes made:
## 1. Only normalize the final scores - avg_time & sd_time & weights are not normalized before computing the final score (weights are already normalized in poi_index).
## 2. Only log normalize the final scores - sd_time is not normalized for correcting skew -> will be in different scales with avg_time in score calculation. Tried not log normalize for the ttm_score, it seems like wider distributed - easier to see the differences, but for nearest 1,2,3, log normalization definitely helps.
## 3. Null weights have been set to 0 - amenities with no info will not add any impact in the numerator of score calculation.
## 4. Set travel times <1 minute to 1 minute - this one basically cause no/very minor change.
## 5. The impact of weights are not obvious - I tried (1+weight) in the numerator, the results are very similar - shall we change the normalization range of weights to increase the impact? 
## 6. Still not sure how to make neareat 1,2,3 comparable? - does nearest 1 suppose to have highest score?? probably only comparable internally??


## Import libraries
```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(ggplot2)

# For pretty knitting
library(lemon)
knit_print.data.frame <- lemon_print
knit_print.tbl <- lemon_print
knit_print.summary <- lemon_print
```

## Import Scoring Functions
```{r}
source('Score_Functions.R')
```

```{r}
# function to plot score distributions by type
plot_densities <- function(score_frame1, score_frame2, titl1, titl2) {
  x <- score_frame1 %>%
        ggplot(aes(x = score, color = type)) +
        geom_density() +
        egg::theme_article() +
        theme(aspect.ratio = 0.3) +
        ggtitle(titl1)
  y <- score_frame2 %>%
        ggplot(aes(x = score, color = type)) +
        geom_density() +
        egg::theme_article() +
        theme(aspect.ratio = 0.3)+
        ggtitle(titl2)
  gridExtra::grid.arrange(x, y)
}

```


## Import data 
```{r kable.opts=list(caption='Summary Table')}

## Import raw Travel Time Matrix (ttm)
ttm <- read.csv('ttm.csv')

n_origins <- 15197 # known origins
n_amenities <- 432 # known destinations from considered amenities

paste('Origins considered:', round(length(unique(ttm$fromId))/n_origins*100, 2), '%')
paste('Destinations considered:', round(length(unique(ttm$toId))/n_amenities*100, 2), '%')
paste('Rows = ', nrow(ttm))

# convert Ids from double to factor
ttm$fromId <- as.factor(ttm$fromId)
ttm$toId <- as.factor(ttm$toId)

summary(ttm[,3:4])
```

## Data Wrangling 

**Wrangling Notes:**
- Remove skews and extreme values
- Due to the diversity in amenity types (which all serve a unique cultural purpose), we'll consider them independently for accessibility score computations.
- Amenities which were interested in studying have already been filtered out in the ttm computation. They are the following:
  - Museums
  - Libraries
  - Galleries
  - Theatres

**Import and join amenity types**

```{r kable.opts=list(caption='Summary Table')}

target_amenities <- c('gallery', 'museum', 'library or archives', 'theatre/performance and concert hall')
amenities <- read.csv('vancouver_facilities_2.csv') %>% filter(type %in% target_amenities)

# preview original
sample_n(amenities, 3)

# clean
amenities <- amenities[,c(1,4)] # only need id and type columns
amenities$id <- as.factor(amenities$id)     # convert to factor
amenities$type <- as.factor(amenities$type) # convert to factor

# preview clean
sample_n(amenities, 3)

# view summary
amenities %>% group_by(type) %>% summarise(count = n()) %>% arrange(desc(count))

ttm <- ttm %>% left_join(amenities, by = c('toId' = 'id'))

names(ttm)[names(ttm) == 'avg_unique_time'] <- "avg_time"
names(ttm)[names(ttm) == 'sd_unique_time'] <- "sd_time"

summary(ttm[,3:4])
sample_n(ttm, 5)

par(mfrow = c(1,2))
plot(density(ttm[,3]), main = 'Travel Time (Density)')
plot(density(ttm[,4]), main = 'Std Dev of Travel Time (Density)')
```


**Replace travel times less than 5 minutes to 5 minutes**

This is done to prevent infinity values in the scoring. Normalization will be done to prevent zero values but it still creates a largely skewed score if we include travel times that approach zero. 5 minutes is also a realistic time window for any travel time that may take 0 - 5 minutes.

```{r}
par(mfrow = c(1, 2))

hist((ttm$avg_time), xlab = 'Original Travel Time', main = '',
     xlim = c(0, 25), ylim = c(0, 120000))

# set travel times <1 minute to 1 minute
min_1min <- pmax(ttm$avg_time, 1)
hist(min_1min, xlab = 'Original Travel Time', main = '',
     xlim = c(0, 25), ylim = c(0, 120000))

ttm$avg_time <- min_1min
```

## Add Amenity Weights

```{r kable.opts=list(caption='Summary Table')}

# Import weight
dest_wts <- read.csv('poi_index.csv')

# clean
dest_wts <- dest_wts[, c(6,7)] # keep weight, id
names(dest_wts) <- c('weight', 'id')
dest_wts$id <-  as.factor(dest_wts$id)
head(dest_wts)

# see weight distribution
plot(density(dest_wts$weight), main = 'Amenity Popularity Distribution')

# join column
ttm_wts <- left_join(ttm, dest_wts, by = c('toId'='id'))

# If any weights are undefined replace with 0
ttm_wts$weight[is.na(ttm_wts$weight)] <- min(ttm_wts$weight)

head(ttm_wts)
range(dest_wts$weight)
```

## Sum Scoring Method

```{r message=FALSE, warning=FALSE}

na.omit(ttm_wts)->ttm_wts

```
## Sum Scoring Method 2 with mean plus sd

```{R}
ttm_scores_2 <- sum_score_fxn_2(ttm_wts,
                            weight = FALSE,
                            log_normalize_score = FALSE)

ttm_wtd_scores_2 <- sum_score_fxn_2(ttm_wts,
                            weight = TRUE,
                            log_normalize_score = FALSE)
```

```{r}
par(mfrow=c(1,2))
plot_densities(ttm_scores_2, ttm_wtd_scores_2, 'Unweighted Scores with 1/(Mean + 2*Sd)', 'Weighted Scores with 1/(Mean+2*Sd)')

```



## Sum Scoring for the Nearest 1, 2, or 3 Amenities

*Note that for nearest 1, the sum is the value itself.*

```{r message=FALSE, warning=FALSE}

# Keep only the nearest 1, 2, or 3 travel times for each dissemination block

nearest_1_ttm <- ttm_wts %>%
                  group_by(fromId, type) %>%
                  summarise(avg_time = min(avg_time), 
                            sd_time = sd_time[which.min(avg_time)],
                            weight = weight[which.min(avg_time)])

nearest_2_ttm <- ttm_wts %>%
                  group_by(fromId, type) %>%
                  summarise(avg_time = na.omit(sort(avg_time)[1:2]), 
                            sd_time = sd_time[which(na.omit(avg_time == sort(avg_time)[1:2]))], 
                            weight = weight[which(na.omit(avg_time == sort(avg_time)[1:2]))])

nearest_3_ttm <- ttm_wts %>%
                  group_by(fromId, type) %>%
                  summarise(avg_time = na.omit(sort(avg_time)[1:3]), 
                            sd_time = sd_time[which(na.omit(avg_time == sort(avg_time)[1:3]))], 
                            weight = weight[which(na.omit(avg_time == sort(avg_time)[1:3]))])

```

### using score function of 1/(mean+2sd)


```{r}
# scores by nearest amenities

n1_ttm_score_2 <- sum_score_fxn_2(nearest_1_ttm, weight = FALSE, log_normalize_score = TRUE)
n1_wt_ttm_score_2 <- sum_score_fxn_2(nearest_1_ttm, weight = TRUE, log_normalize_score = TRUE)

n2_ttm_score_2 <- sum_score_fxn_2(nearest_2_ttm, weight = FALSE, log_normalize_score = TRUE)
n2_wt_ttm_score_2 <- sum_score_fxn_2(nearest_2_ttm, weight = TRUE, log_normalize_score = TRUE)

n3_ttm_score_2 <- sum_score_fxn_2(nearest_3_ttm, weight = FALSE, log_normalize_score = TRUE)
n3_wt_ttm_score_2 <- sum_score_fxn_2(nearest_3_ttm, weight = TRUE, log_normalize_score = TRUE)

```


```{r}
plot_densities(n1_ttm_score_2, n1_wt_ttm_score_2, 'Unweighted Scores with 1/(Mean + 2*Sd)', 'Weighted Scores with 1/(Mean+2*Sd)')
plot_densities(n2_ttm_score_2, n2_wt_ttm_score_2, 'Unweighted Scores with 1/(Mean + 2*Sd)', 'Weighted Scores with 1/(Mean+2*Sd)')
plot_densities(n3_ttm_score_2, n3_wt_ttm_score_2, 'Unweighted Scores with 1/(Mean + 2*Sd)', 'Weighted Scores with 1/(Mean+2*Sd)')

```

## Exporting all Score Sets

```{r}

## Add weight column for each score frame

ttm_scores_2$weight <- as.factor('no')
ttm_wtd_scores_2$weight <- as.factor('yes')

n1_ttm_score_2$weight <- as.factor('no')
n1_wt_ttm_score_2$weight <- as.factor('yes')

n2_ttm_score_2$weight <- as.factor('no')
n2_wt_ttm_score_2$weight <- as.factor('yes')

n3_ttm_score_2$weight <- as.factor('no')
n3_wt_ttm_score_2$weight <- as.factor('yes')

## Add nearest_n column for each score frame

ttm_scores_2$nearest_n <- as.factor('all')
ttm_wtd_scores_2$nearest_n <- as.factor('all')

n1_ttm_score_2$nearest_n <- as.factor('1')
n1_wt_ttm_score_2$nearest_n <- as.factor('1')

n2_ttm_score_2$nearest_n <- as.factor('2')
n2_wt_ttm_score_2$nearest_n <- as.factor('2')

n3_ttm_score_2$nearest_n <- as.factor('3')
n3_wt_ttm_score_2$nearest_n <- as.factor('3')

## Combine into a long dataframe
all_scores <- list(ttm_scores_2, ttm_wtd_scores_2,
                   n1_ttm_score_2, n1_wt_ttm_score_2,
                   n2_ttm_score_2, n2_wt_ttm_score_2,
                   n3_ttm_score_2, n3_wt_ttm_score_2)

long_scores <- data.table::rbindlist(all_scores) %>% arrange(fromId)

## Re-Order columns
long_scores <- long_scores[, c(1, 2, 4, 5, 3)]

## Export
write.csv(long_scores, 'new_modified_long_scores.csv', row.names = FALSE)
```

